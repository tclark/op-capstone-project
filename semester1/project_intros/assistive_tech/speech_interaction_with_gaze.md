**Extending speech interaction with gaze interaction **

**Overview**

This project has the end goal of creating a multimodal system that
enhances the speech interaction by adding gaze commands

**Background**

users with motor impairment find it difficult to interact with
electronic devices. Speech interaction circumvents some of the problems
faced by users with motor disability . However, in many situations
speech only interaction is far from ideal. This project proposes to
enhance speech interaction with gaze interaction capabilities. That is,
the ability to interact with objects in a user interface by gazing at
them. The information about the users gaze can be provided by a hardware
device known as an eye tracker.

**Deliverables**

a plug-in module for existing speech interaction software such as Dragon
NaturallySpeaking, Windows speech recognition or Google speech API that
enhances its functionality by adding gaze interaction functionality such
as:

-   turning on/off the microphone via gaze hotspots

-   correcting misrecognized words via gaze hotspots

-   etc.
